Implementation Plan for MCP Servers Metadata and Playground Features

1. Automating Metadata & Documentation Generation (OpenAI-Powered)

Overview: Leverage OpenAI’s API to analyze each MCP server’s GitHub repository (code, README, topics) and auto-generate missing metadata and user documentation. This logic will run in the backend (e.g. a function under lib/verification or lib/github) after a server is validated.
	•	Extract Repository Info: Use GitHub APIs or direct retrieval to gather basic info:
	•	Primary language (from repo language stats or file analysis).
	•	Existing description (e.g. from README introduction).
	•	Author/owner (repo owner or author name in README/license).
	•	GitHub topics/tags for context (e.g. categories, framework).
	•	Generate Missing Metadata: If any fields are missing or inadequate, call OpenAI to summarize or infer them:
	•	Prompt the model with the repo’s README and key code snippets to get a concise description and identify the main language/framework.
	•	Example: “Here is a README and code outline of a project. Provide the primary language, a one-sentence description, and the author or organization.”
	•	Generate Detailed Documentation: For user-facing docs, feed more of the repository content to OpenAI:
	•	Request a usage guide or API documentation in markdown. For example, ask the model to produce a README section explaining how to deploy and use the server, based on the repo’s content.
	•	This approach is inspired by LLM pipelines that “ingest all the files in [a] GitHub repo, [ask] the LLM to generate descriptive markdown documentation” ￼. The model can synthesize a guide from the code and comments.
	•	Storage of Results: Save the AI-generated outputs in persistent storage:
	•	Metadata (language, description, author, etc.) – save as structured attributes in DynamoDB. These are small, so they fit within DynamoDB’s item size limits.
	•	Documentation (detailed usage docs) – this could be several KBs of text. Rather than storing a large blob in Dynamo, follow best practice by uploading it to Amazon S3 and storing only a reference (URL or S3 key) in DynamoDB ￼ ￼. This keeps Dynamo items small and costs low, while the full doc is retrievable via S3.
	•	DynamoDB Updates: Extend the MCP servers table schema to include new fields like language, description, author, and docUrl (link to the S3 documentation object). DynamoDB’s schemaless nature means we can add attributes without heavy migrations, but update any TypeScript interfaces/models to reflect these new fields.
	•	Security & Quotas: The OpenAI API key should be stored securely (e.g. in AWS Secrets Manager or environment variable) and the generation function should handle API rate limits or errors gracefully. Generate documentation once (after validation) and store it, rather than on every user request, to control API usage.

Prompt Implementation (Cursor): To implement this feature, we would create a backend function and test it:
	•	Prompt: “Create a new module lib/verification/autoGenerateMetadata.ts with a function generateMetadataAndDocs(repoUrl) that uses OpenAI. It should fetch the repo’s README and key info, then call the OpenAI API to generate a JSON object with language, description, author, etc., and a markdown documentation string. The function should then save the metadata fields to DynamoDB (table McpServers) and the documentation string to an S3 bucket (e.g. mcp-docs), returning the S3 URL. Use AWS SDK v3 for DynamoDB and S3. Ensure to handle errors (e.g., OpenAI failures) and log appropriately.”
	•	Test Prompt: “Using a test repository (with known README content), call generateMetadataAndDocs and assert that it returns an object containing the language (e.g. “Python”), a non-empty description, and an S3 URL. Verify that the DynamoDB item for that repo ID now contains the new fields and that the S3 object exists (you can simulate the S3 put by mocking the S3 client in a unit test).”

2. “Try in Playground” Container Launch Functionality

Overview: Allow users to spin up a validated MCP server on-demand in an interactive “playground” mode. When a user clicks “Try in Playground,” the system will launch the server’s container (previously used during validation) so the user can interact with it live. We will evaluate two approaches for triggering the container:
	•	Approach (a) – Next.js API Route Trigger: Implement a Next.js API endpoint (e.g. /api/playground/[serverId]) that, when called, uses AWS SDK to launch the container:
	•	Use ECS’s RunTask API to start a new task for the server’s ECS task definition (the same used in validation). This can be done synchronously in the API route. The API should respond quickly, so it might kick off the task and immediately return a status (e.g. “launching”) along with an identifier or the task ARN.
	•	Next.js API route will run on the server side with AWS credentials (ensure the execution role or keys have permission for ecs:RunTask on the cluster). This direct trigger is straightforward and keeps logic in one place (the web app backend).
	•	After launching, store the taskArn and maybe the public endpoint info in DynamoDB (e.g. update the server’s record with a playgroundTaskArn and timestamp). This allows tracking the running session.
	•	Optionally, poll for the task’s public IP or DNS once it’s running (using ECS DescribeTasks). The API could return the connection info (e.g. URL or IP:port) so the frontend can inform the user. (In MVP, we might just instruct the user to check logs or assume a default URL pattern.)
	•	Approach (b) – Reuse Existing Launcher Logic: If the validation process already has an ECS/Fargate launch mechanism (perhaps in a pipeline or AWS Step Function), we could refactor it to support an “interactive” mode:
	•	For example, if validation uses a Step Function or CodeBuild to run the container and test it, we can add a branch or flag to run the container without tests. The Next.js frontend would then invoke this pipeline (via an API or message) to launch the container in interactive mode.
	•	This ensures we reuse any existing setup (networking, task definitions, logging) and maintain consistency. The pipeline could return the task info to the frontend when ready.
	•	However, this approach adds complexity and latency (invoking a pipeline or secondary service). It may be better for long-term maintainability if the pipeline is robust, but for immediate implementation, the direct API trigger (a) is simpler.
	•	Auto-Termination for Cost Efficiency: Regardless of trigger approach, it’s critical that playground containers do not run indefinitely. AWS Fargate charges for the entire runtime of the task (it “charges per resources ‘configured’ not ‘used’”, meaning an idle container still incurs cost) ￼. To save costs:
	•	Short Lifespan: Launch the playground task with an expectation to run only a short duration (e.g. 15-30 minutes). We can enforce this by scheduling a stop. For instance, record the start time, and use a background scheduler (or a CloudWatch Events rule) to call StopTask after a timeout unless the user requests extension.
	•	Inactivity Detection: If possible, have the container itself detect inactivity (no requests) and self-terminate. In practice, implementing a reliable idle timer inside the container may be complex. Instead, an external watcher can monitor metrics or logs. For example, a known on-demand server approach starts a container when needed and “stops when idle, helping reduce costs by running only when needed.” ￼ We can mimic this by checking if the playground has had any user activity (maybe via a heartbeat API call from the frontend) and stop it after X minutes of no activity.
	•	Resource Limits: Run the playground task on minimal CPU/Memory (just enough to function) to reduce cost per minute. Since it’s not for heavy production load, a small Fargate size is sufficient.
	•	User Access: Ensure the user can access the running container:
	•	The task should run in our AWS environment with either a public IP or behind a load balancer. For a quick solution, assign a public IP to the task (ECS Fargate supports this with awsvpc network mode) and open the necessary port. The frontend can display this IP or route it through our domain (e.g., by CNAME or a simple proxy if needed).
	•	Security group rules might allow access from anywhere (since users could be anywhere), but if needed, we could restrict by source IP or through an authenticated proxy. Initially, open access on the specific port (the container’s service port) for simplicity.
	•	Cleanup: When the user ends the session or the timeout hits, stop the ECS task via AWS SDK (StopTask). Also, update DynamoDB to remove or mark the playgroundTaskArn as terminated. If using approach (b), the pipeline or Fargate itself might handle cleanup, but with approach (a) we must explicitly stop it.

Prompt Implementation (Cursor): We will implement the trigger and ensure auto-stop:
	•	Prompt: “Create a Next.js API route at pages/api/playground/[id].ts that accepts a server ID. In the handler, lookup the server in DynamoDB (to get its image or task definition). Use AWS ECS SDK to runTask for the server’s task definition (launch type Fargate, cluster from env). Pass an environment variable like MODE=playground to instruct the container it’s in interactive mode (if needed). Store the taskArn and start time in DynamoDB for that server ID. Respond with a JSON containing the taskArn and a status.”
	•	Test Prompt: “Simulate a user clicking ‘Try in Playground’ by calling the API route for a known server ID (you can stub the ECS call to return a fake taskArn). Verify the API response includes a taskArn and that the DynamoDB record for that ID now has a playgroundTaskArn and timestamp. Also simulate a stop: call a function (or schedule) to stop the task after, say, 1 minute. Verify that ecs:StopTask is invoked (can be a mock in test) and that the DynamoDB record’s playgroundTaskArn is cleared or marked inactive.”

3. AWS CDK Infrastructure Updates for Playground & Storage

Overview: Update the AWS Cloud Development Kit (CDK) stack to provision resources needed for the new features and adjust IAM roles. These changes will be made in the infrastructure code (e.g. in ValidationPipelineStack.ts or a new stack if appropriate):
	•	S3 Bucket for Documentation: Add an S3 bucket to store generated documentation:
	•	Define a new S3 Bucket resource (e.g. name it mcp-server-docs). Enable server-side encryption and perhaps versioning (to retain updates of docs).
	•	Set an appropriate retention policy (probably keep data, since docs are not huge).
	•	Permissions: Grant write access to this bucket from the component that will generate docs. For example, if the metadata generator runs in a Lambda or ECS task, ensure its IAM role includes s3:PutObject (and maybe s3:GetObject if we need to read back) for the bucket. Similarly, allow the web frontend to read the docs (if the docs will be fetched via backend, its role needs read, or if directly by users via a presigned URL, the backend can generate that).
	•	(No need to make the bucket public; use presigned URLs or proxy through the backend for security. The DynamoDB will store the S3 key or URL, which the frontend can use to retrieve when needed.)
	•	DynamoDB Schema Adjustments: If using provisioned throughput or indexes that include the new attributes, update those configurations in CDK:
	•	Typically, simply adding new attributes doesn’t require schema changes for Dynamo (they are added dynamically). But if we want to query by language or other fields, we might define a Global Secondary Index. For now, assume no new GSI needed – data will be fetched by primary key (server ID).
	•	Update any AWS CDK definitions for the table if necessary (e.g., increasing attribute capacity if expecting larger item size due to new fields – however, offloading docs to S3 keeps items small).
	•	ECS Task Definition / Cluster for Playground: Ensure the ECS infrastructure can support on-demand tasks:
	•	If a cluster and task definition for validation already exist, reuse them. The task definition might have to be adjusted to allow an override in command or environment for interactive mode. For example, include an environment variable in the container definition like MODE with default “validation”, and our playground launch will override it to “interactive”. This way the container knows not to auto-exit after tests.
	•	If the validation was using AWS CodeBuild or another mechanism to run tests, we might need to create a new ECS Task Definition for running the server in normal mode. In CDK, define this task (container image, CPU/memory, port mappings, etc.).
	•	The task should be able to have a public IP. In CDK, if using Fargate, set assignPublicIp: true in the Fargate task options (and ensure the cluster’s VPC has a subnet with Internet access). Also, create a Security Group for playground tasks that allows inbound traffic on the server’s port (e.g. 80 or 8080).
	•	Optionally, set up an Application Load Balancer listener and target group for these tasks if we want to route via a friendly URL. But since tasks are ephemeral, a simpler route is to use the public IP approach. We can later integrate a load balancer for more control.
	•	IAM Roles and Permissions:
	•	Playground Launcher: The Next.js API (likely running as a Lambda in AWS or an EC2-based service) needs permission to launch and stop tasks. In CDK, locate the IAM role for the API (or wherever the backend code runs) and attach an IAM policy with ecs:RunTask, ecs:StopTask, and iam:PassRole (to pass the task execution role) for the specific cluster/task. Lock it down to the cluster ARN and task definition ARN for security.
	•	Task Execution Role: If the ECS task (the MCP server container) needs to call AWS (e.g., access DynamoDB or other services during validation or for logging), ensure its task role has needed permissions. For interactive mode, it might not need any new rights unless it reports back or writes logs. We should review if the container writes to Dynamo (maybe not).
	•	OpenAI Integration: If using AWS Lambda for the OpenAI call, ensure that lambda has internet access (to reach OpenAI) and access to the OpenAI API key (e.g., permission to read Secrets Manager if the key is stored there).
	•	Update ValidationPipelineStack: In ValidationPipelineStack.ts, incorporate the above:
	•	Instantiate the S3 bucket resource and output its name (so the app knows where to put docs).
	•	Modify the DynamoDB table resource (if needed for indexes or throughput).
	•	Adjust the ECS cluster or task definitions: possibly add a new Fargate task definition for interactive sessions if different from validation tasks.
	•	If validation currently uses Fargate tasks as well, ensure the cluster scaling can handle on-demand tasks (might just be Fargate On-Demand which will scale automatically).
	•	Add any new environment variables to the Next.js API lambda function (e.g. bucket name, cluster name, task definition ARN, etc., so the code can use them).
	•	The CDK stack should also include any necessary CloudWatch alarm or EventBridge rule for stopping idle tasks. For example, we could define an EventBridge rule that triggers a Lambda to check for tasks older than X minutes and stop them. This can be an advanced enhancement; initially we might rely on the backend logic itself to schedule the stop.
	•	Deploy and Verify: After updating CDK, deploy the stack:
	•	Verify the S3 bucket is created and the DynamoDB table still works with new attributes.
	•	Ensure the ECS cluster and task definitions are configured for public accessibility in playground mode.
	•	Check IAM roles: the API’s role should have the new permissions to ECS and S3, and no permission errors occur when actually running the features.

Prompt Implementation (Cursor): Steps to modify infrastructure as code and test:
	•	Prompt: “Open infrastructure/ValidationPipelineStack.ts. Add a new S3 Bucket resource named DocsBucket for storing documentation. Grant write permissions on this bucket to the verification lambda (if docs generated during validation) or to the application backend role. Next, locate the ECS cluster and task definition setup: enable assignPublicIp for Fargate tasks and open the security group for port 80 to the internet (0.0.0.0/0) for now. Attach an IAM policy to the Next.js API Lambda’s role allowing it to call ecs:RunTask and ecs:StopTask on our cluster (restrict by resource ARNs). After these changes, run cdk synth to ensure no syntax errors and that the new resources (S3 bucket, policy changes) are reflected.”
	•	Test Prompt: “After deploying the CDK stack, verify the new resources: List S3 buckets to see mcp-server-docs exists. Check the IAM role of the Next.js API (e.g. in AWS IAM console) to confirm it has an inline policy for ECS access. Simulate a run: call the Playground API to launch a task and then use aws ecs describe-tasks with the cluster to ensure the task is running. Also, upload a test file to the new S3 bucket (e.g., via the AWS SDK in a sandbox) to confirm write permissions. Lastly, verify that a sample documentation generation writes to S3 successfully (this can be done by invoking the generateMetadataAndDocs function in a dev environment and checking the S3 object).”

4. Cursor Prompt Series for Implementation and Verification

Finally, we outline the Cursor prompt sequence to build and verify these features step by step. Each prompt to the Cursor AI will guide it to implement a part of the plan, followed by a test to validate the addition:
	1.	Implement OpenAI Metadata/Docs Generator – Prompt: Implement the backend function to fetch repo data and call OpenAI, then store results in DynamoDB/S3 (as described in Section 1). Test: After coding, run a unit test with a sample repo input to ensure it returns filled metadata and that the S3 upload function is called (you might mock OpenAI and S3 for the test).
	2.	Add “Try in Playground” API Endpoint – Prompt: Create the Next.js API route to launch ECS tasks for a given server. Include logic to pass an interactive mode flag and save the task ARN in DB. Test: Invoke the API route in a dev environment (or a unit test mocking ECS) with a dummy server ID. Check that the response contains a fake task ARN and that the function attempted to write to DynamoDB. Also simulate calling a stop function or ensure a timer would trigger as expected (perhaps by calling the stop logic directly in a test).
	3.	Update DynamoDB Schema and Models – Prompt: Update any TypeScript types or database access code to add new fields (language, description, author, docUrl, playgroundTaskArn). Ensure that these fields are handled in create/update operations. Test: Run the application’s existing test suite or a specific test to save a new MCP server record and retrieve it, verifying that the new fields persist. Also test that the generateMetadataAndDocs function from step 1 correctly writes docUrl and that the value can be read back.
	4.	Infrastructure as Code Changes (CDK) – Prompt: Modify the CDK stack to add the S3 bucket and required IAM permissions, and adjust ECS settings for playground tasks (per Section 3). Test: Execute cdk synth to ensure the template is correct. In a staging AWS environment, deploy the changes and then trigger a full flow: run validation on a sample repo (to generate docs), then call “Try in Playground” and confirm the container comes up accessible, then is terminated after timeout. Verify through AWS console that the S3 document is in the bucket and the ECS task appeared and then stopped.

By following this implementation plan and using a series of focused development prompts with verification at each step, we can confidently add the metadata/documentation automation and interactive playground functionality to the MCP server platform. These changes will enrich the user experience with better information and hands-on trials, while the infrastructure updates ensure the solution remains secure and cost-effective (storing large docs in S3 per best practice ￼ and stopping idle containers to avoid needless Fargate charges ￼ ￼).